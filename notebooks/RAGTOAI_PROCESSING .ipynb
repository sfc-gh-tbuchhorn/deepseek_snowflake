{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "cnq3ilcepodx5sqbrlcr",
   "authorId": "298731385241",
   "authorName": "JOHN",
   "authorEmail": "tim.buchhorn@snowflake.com",
   "sessionId": "bd5a253e-1167-4e81-8475-8bb04894ed3c",
   "lastEditTime": 1756179442018
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c3f90f-3b6b-4945-9e22-afff68cf1b4b",
   "metadata": {
    "name": "intro",
    "collapsed": false,
    "resultHeight": 285
   },
   "source": "RAGs to AI Riches\n========\n\nCreating an end-to-end Retrieval Augmented Generation process (or RAG) directly in Snowflake.\n1) Extract full text from PDF files using Snowpark.\n2) Chunk those documents using Langchain in Snowpark.\n3) Use a service hosted on SPCS to create embeddings of those chunks (seperate workbook to set up service).\n4) Use Vector Similarity to show the most similar chunk when prompting an LLM.\n5) Call a DeepSeek LLM hoset on SPCS for context\n\n![Alt text](https://filedn.eu/ljfsfeYp02Sjg88j4jWtPqL/images/ai_llms.webp \"slide1\" )  \n"
  },
  {
   "cell_type": "code",
   "id": "d9f75462-aac3-481c-94df-6b2910abfb1c",
   "metadata": {
    "language": "sql",
    "name": "set_context"
   },
   "outputs": [],
   "source": "USE ROLE DEEPSEEK_ROLE",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\n\n# Snowpark\nfrom snowflake.snowpark.context import get_active_session\nimport snowflake.snowpark.functions as F\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "functions_parse_chunk",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "create or replace function pdf_text_chunker(file_url string)\nreturns table (chunk varchar)\nlanguage python\nruntime_version = '3.9'\nhandler = 'pdf_text_chunker'\npackages = ('snowflake-snowpark-python','PyPDF2', 'langchain')\nas\n$$\nfrom snowflake.snowpark.types import StringType, StructField, StructType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom snowflake.snowpark.files import SnowflakeFile\nimport PyPDF2, io\nimport logging\nimport pandas as pd\n\nclass pdf_text_chunker:\n\n    def read_pdf(self, file_url: str) -> str:\n    \n        logger = logging.getLogger(\"udf_logger\")\n        logger.info(f\"Opening file {file_url}\")\n    \n        with SnowflakeFile.open(file_url, 'rb') as f:\n            buffer = io.BytesIO(f.readall())\n            \n        reader = PyPDF2.PdfReader(buffer)   \n        text = \"\"\n        for page in reader.pages:\n            try:\n                text += page.extract_text().replace('\\n', ' ').replace('\\0', ' ')\n            except:\n                text = \"Unable to Extract\"\n                logger.warn(f\"Unable to extract from file {file_url}, page {page}\")\n        \n        return text\n\n    def process(self,file_url: str):\n\n        text = self.read_pdf(file_url)\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 4000, #Adjust this as you see fit\n            chunk_overlap  = 400, #This let's text have some form of overlap. Useful for keeping chunks contextual\n            length_function = len\n        )\n    \n        chunks = text_splitter.split_text(text)\n        df = pd.DataFrame(chunks, columns=['chunks'])\n        \n        yield from df.itertuples(index=False, name=None)\n$$;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "38858d47-9408-4857-9a5b-f32d6c93e85f",
   "metadata": {
    "language": "sql",
    "name": "listthem",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- add pdfs to the stage\n\nls @docs;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7718bacf-6fa2-4ddf-b44c-5c1cc2b748b7",
   "metadata": {
    "name": "About_Chunking",
    "collapsed": false,
    "resultHeight": 220
   },
   "source": "A note on chunking\n-----\nChunking is the process of splitting a large body of text into smaller 'chunks' whilst attempting to keep as much relevant information as possible. Make the chunks too small and you run the risk of removing key information that the model requires to answer the question. Too large and it may be harder to retreive the correct body of text from the vector search - or spend tokens excessively.\n\nThere are many strategies towards chunking. Eg - pass the most relevant, top n relevant chunks, or pass the most relevent chunk + the chunk either side of that one. Play around and see what works for your use case!"
  },
  {
   "cell_type": "code",
   "id": "5103601a-0307-4a83-9350-f45ea4330b9c",
   "metadata": {
    "language": "sql",
    "name": "chunkem",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- create chunks\n\ncreate or replace TABLE DOCS_CHUNKS_TABLE ( \n    RELATIVE_PATH VARCHAR(16777216), -- Relative path to the PDF file\n    SIZE NUMBER(38,0), -- Size of the PDF\n    FILE_URL VARCHAR(16777216), -- URL for the PDF\n    SCOPED_FILE_URL VARCHAR(16777216), -- Scoped url (you can choose which one to keep depending on your use case)\n    CHUNK VARCHAR(16777216));  -- Embedding using the VECTOR data type\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f06e08e7-84a5-4a18-89ed-9b86015bdbc3",
   "metadata": {
    "language": "python",
    "name": "docs_df"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col, lit, sql_expr\n\n# --- Load file listing from stage ---\ndocs_stage = \"@docs\"\ndirectory_df = session.sql(f\"SELECT * FROM directory('{docs_stage}')\").with_column(\n    \"scoped_file_url\",\n    sql_expr(f\"build_scoped_file_url('{docs_stage}', RELATIVE_PATH)\")\n)\ndirectory_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e825494a-0ccf-40c3-bfdd-5afdb7d7c0dd",
   "metadata": {
    "language": "python",
    "name": "chunk_and_save"
   },
   "outputs": [],
   "source": "chunk_df = directory_df.select(\n    col(\"relative_path\"),\n    col(\"size\"),\n    col(\"file_url\").alias(\"file_url\"),\n    col(\"scoped_file_url\")\n).join_table_function(\n    \"pdf_text_chunker\",\n    col(\"scoped_file_url\")\n)\n\nchunk_df = chunk_df.with_column(\"CONTEXT\", F.col(\"chunk\"))\n\nchunk_df.write.mode('overwrite').save_as_table('RAG_CHUNKED_TABLE')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7be7513d-b635-4b26-92aa-917bf8f4c6f3",
   "metadata": {
    "language": "python",
    "name": "reload_chunks"
   },
   "outputs": [],
   "source": "chunk_df = session.table(\"RAG_CHUNKED_TABLE\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7aa2da2c-d856-4ff9-abcc-3671e881dc2d",
   "metadata": {
    "language": "sql",
    "name": "show_chunks_table"
   },
   "outputs": [],
   "source": "SELECT * FROM RAG_CHUNKED_TABLE LIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d3298da-7fbc-4ca2-acd0-b0b43f9f3aa7",
   "metadata": {
    "name": "embedding_title",
    "collapsed": false
   },
   "source": "Now we check the embedding service running on SPCS"
  },
  {
   "cell_type": "code",
   "id": "0453ab73-1973-4c8d-b49f-fb82a75da0ac",
   "metadata": {
    "language": "sql",
    "name": "show_embed_services"
   },
   "outputs": [],
   "source": "-- Run this to check whether status = RUNNING\nSHOW SERVICES IN COMPUTE POOL GPU_NV_S_COMPUTE_POOL;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca04614b-b173-45d9-a243-7c8fe2821c4f",
   "metadata": {
    "language": "python",
    "name": "registry_object"
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\n\n# Create Model Registry\nreg = Registry(\n    session=session, \n    database_name=session.get_current_database(), \n    schema_name=\"EMBEDDING_MODEL_HOL_SCHEMA\"\n    )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c94b1d4d-b162-4995-bb44-a35dcb7c902c",
   "metadata": {
    "language": "python",
    "name": "show_reg_models"
   },
   "outputs": [],
   "source": "reg.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d50c9c1-fd8e-4b2d-9df1-ed36163a98ed",
   "metadata": {
    "language": "python",
    "name": "load_model"
   },
   "outputs": [],
   "source": "mv = reg.get_model('sentence_transformer_minilm').version('V1')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "86216f46-b0f6-4f31-b53b-18b4c0cf8184",
   "metadata": {
    "language": "python",
    "name": "embed_chunks"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import functions as F\nfrom snowflake.snowpark import types as T\n\n# --- Embed the chunks using the model registry function ---\n# If your model is deployed via container service:\nembedded_chunk_df = mv.run(\n    chunk_df, \n    function_name=\"encode\",\n    service_name = 'minilm_gpu_service'\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d6851f2-5f84-43d0-8596-dc6652329d32",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": "embedded_chunk_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c843711d-f340-4f41-a22f-6545e48dcf15",
   "metadata": {
    "language": "python",
    "name": "vector_type"
   },
   "outputs": [],
   "source": "embedded_chunk_df = embedded_chunk_df.with_column('\"output_feature_0\"', F.col('\"output_feature_0\"').cast(T.VectorType(float, 384)))\nembedded_chunk_df = embedded_chunk_df.rename(F.col('\"output_feature_0\"'), \"CHUNK_VEC\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b44fbae4-80e1-4899-97e3-108f6a5ec16f",
   "metadata": {
    "language": "python",
    "name": "show_embedded_vectors"
   },
   "outputs": [],
   "source": "embedded_chunk_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eb74f49a-e031-44ab-8e03-acbdcefe7da1",
   "metadata": {
    "language": "python",
    "name": "save_embedded_chunks"
   },
   "outputs": [],
   "source": "embedded_chunk_df.write.mode('overwrite').save_as_table('RAG_CHUNKED_EMBEDDED_TABLE')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53915616-39ef-4ce6-adc5-487062ee025d",
   "metadata": {
    "language": "sql",
    "name": "load_embedded_chunks"
   },
   "outputs": [],
   "source": "SELECT * FROM RAG_CHUNKED_EMBEDDED_TABLE LIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ce4bd42-2b44-44d6-9545-bcdad6672592",
   "metadata": {
    "language": "python",
    "name": "embed_prompt"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Row\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark.types import StructType, StructField, StringType\n\nuser_input = \"Tell me about the Xtreme road bike\"\n\n# Build one-row DF with required input column\ninput_df = session.create_dataframe(\n    [Row(CONTEXT=user_input)],\n    schema=StructType([StructField(\"CONTEXT\", StringType())])\n)\n\nprompt_embedded_chunk_df = mv.run(\n    input_df,\n    function_name=\"encode\",\n    service_name=\"minilm_gpu_service\"\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a0117fd-3bb7-4731-92ab-07da1b6ef0b8",
   "metadata": {
    "language": "python",
    "name": "vectorize_embedded_prompt"
   },
   "outputs": [],
   "source": "prompt_embedded_chunk_df = prompt_embedded_chunk_df.with_column('\"output_feature_0\"', F.col('\"output_feature_0\"').cast(T.VectorType(float, 384)))\nprompt_embedded_chunk_df = prompt_embedded_chunk_df.rename(F.col('\"output_feature_0\"'), \"CHUNK_VEC\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5fc73e36-e20a-44e8-873d-80aa2cbb11bc",
   "metadata": {
    "language": "python",
    "name": "save_vectorized_prompt"
   },
   "outputs": [],
   "source": "prompt_embedded_chunk_df.write.mode('overwrite').save_as_table('QUERY_TABLE')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f44da037-677e-45bd-8cea-c3fdf262efca",
   "metadata": {
    "language": "sql",
    "name": "load_vectorized_prompt"
   },
   "outputs": [],
   "source": "SELECT * FROM QUERY_TABLE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5037ecd7-5e9d-41b3-85e5-7f0abd290d94",
   "metadata": {
    "language": "sql",
    "name": "vector_similarity"
   },
   "outputs": [],
   "source": "-- https://docs.snowflake.com/en/user-guide/snowflake-cortex/vector-embeddings#retrieval-augmented-generation-rag\n\nSELECT\n    r.chunk,\n    VECTOR_COSINE_SIMILARITY(r.chunk_vec, q.chunk_vec) AS similarity\nFROM DEEPSEEK_DB.RAGTOAI.RAG_CHUNKED_EMBEDDED_TABLE r, query_table q\nORDER BY similarity DESC\nLIMIT 1;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5840cf0f-2317-4f18-84d5-230e61c7ffbe",
   "metadata": {
    "language": "python",
    "name": "show_most_similar_chunk"
   },
   "outputs": [],
   "source": "chunk = vector_similarity.to_df().collect()[0][\"CHUNK\"]",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "52579948-101d-4478-9bcd-9783135597d0",
   "metadata": {
    "name": "service_function_explanation",
    "collapsed": false
   },
   "source": "In the next cell we take the chunk and provide it to the DeepSeek model wrapped in a service function. The service function does not support streaming, so it will think about the answer, and return it all at once"
  },
  {
   "cell_type": "code",
   "id": "befd40d3-769d-4dba-bca7-8c1c1f8d3b0b",
   "metadata": {
    "language": "python",
    "name": "context_to_chunk"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import call_udf\n\ncontext_q = f\"\"\"\nAnswer the question based on the context. Be concise.\nContext: {chunk}\nQuestion: {user_input}\nAnswer:\n\"\"\"\n\ndf = session.create_dataframe([[context_q]], schema=[\"prompt\"])\n\nresult_df = df.select(call_udf(\"DEEPSEEK_DB.PUBLIC.DEEPSEEK_CHAT_UDF\", df[\"prompt\"]).alias(\"response\"))\n\nresult = result_df.collect()[0][\"RESPONSE\"]\n\nresult",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e5462a17-12a8-49da-9901-b7c99b160c95",
   "metadata": {
    "name": "streamlit_introduction",
    "collapsed": false
   },
   "source": "Next we take all the steps above and put it in a streamlit app"
  },
  {
   "cell_type": "code",
   "id": "1e3ab979-bc3d-4609-949a-d5e0331cea42",
   "metadata": {
    "language": "python",
    "name": "streamlit_RAG"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Row\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark.types import StructType, StructField, StringType\nimport streamlit as st # Import python packages\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import call_udf\nsession = get_active_session() # Get the current credentials\n\nst.title(\":snowflake: Ask Your Data Anything :snowflake:\")\nst.write(\"\"\"Built using end-to-end RAG in Snowflake with Cortex functions.\"\"\")\n\nprompt = st.text_input(\"Enter prompt\", placeholder=\"What is being discussed in this document?\", label_visibility=\"collapsed\")\n\nif prompt:\n    # Build one-row DF with required input column\n    input_df = session.create_dataframe(\n        [Row(CONTEXT=prompt)],\n        schema=StructType([StructField(\"CONTEXT\", StringType())])\n    )\n    \n    prompt_embedded_chunk_df = mv.run(\n        input_df,\n        function_name=\"encode\",\n        service_name=\"minilm_gpu_service\"\n    )\n    \n    prompt_embedded_chunk_df = prompt_embedded_chunk_df.with_column('\"output_feature_0\"', F.col('\"output_feature_0\"').cast(T.VectorType(float, 384)))\n    \n    prompt_embedded_chunk_df = prompt_embedded_chunk_df.rename(F.col('\"output_feature_0\"'), \"CHUNK_VEC\")\n    \n    prompt_embedded_chunk_df.write.mode('overwrite').save_as_table('QUERY_TABLE')\n    \n    closest_vector_q = '''SELECT\n        r.chunk,\n        VECTOR_COSINE_SIMILARITY(r.chunk_vec, q.chunk_vec) AS similarity\n    FROM DEEPSEEK_DB.RAGTOAI.RAG_CHUNKED_EMBEDDED_TABLE r, query_table q\n    ORDER BY similarity DESC\n    LIMIT 1'''\n    \n    closest_vector = session.sql(closest_vector_q).to_pandas()\n    chunk = closest_vector['CHUNK'].iloc[0]\n\n    context_q = f\"\"\"\n    Answer the question based on the context. Be concise.\n    Context: {chunk}\n    Question: {prompt}\n    Answer:\n    \"\"\"\n\n    df = session.create_dataframe([[context_q]], schema=[\"prompt\"])\n\n    result_df = df.select(call_udf(\"DEEPSEEK_DB.PUBLIC.DEEPSEEK_CHAT_UDF\", df[\"prompt\"]).alias(\"response\"))\n\n    result = result_df.collect()[0][\"RESPONSE\"]\n\n    st.write(result)\n",
   "execution_count": null
  }
 ]
}